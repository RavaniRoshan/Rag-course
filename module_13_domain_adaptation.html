<!DOCTYPE html>

<html lang="en"><head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Academic RAG Journal - Module 13: Domain Adaptation</title>
<!-- Fonts -->
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&amp;family=Inter:wght@400;500;600;700;800&amp;display=swap" rel="stylesheet"/>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&amp;family=Inter:wght@400;500;600;700;800&amp;display=swap" rel="stylesheet"/>
<!-- Tailwind CSS -->
<script src="https://cdn.tailwindcss.com?plugins=forms,container-queries,typography"></script>
<!-- Theme Config -->
<script id="tailwind-config">
        tailwind.config = {
            darkMode: "class",
            theme: {
                extend: {
                    colors: {
                        "bg": "#0E0E10",
                        "bg-secondary": "#18181B",
                        "border": "#27272A",
                        "foreground": "#FAFAFA",
                        "muted": "#A1A1AA",
                        "accent": "#3ECF8E",
                        "accent-secondary": "#24A472",
                        "accent-foreground": "#0E0E10",
                        "destructive": "#C92A2A",
                        "warning": "#F59E0B",
                        "info": "#3B82F6",
                    },
                    fontFamily: {
                        "sans": ["Inter", "sans-serif"],
                        "mono": ["JetBrains Mono", "monospace"],
                    },
                    boxShadow: {
                        'brutal': '4px 4px 0px 0px rgba(62, 207, 142, 0.3)',
                        'brutal-lg': '6px 6px 0px 0px rgba(62, 207, 142, 0.4)',
                    }
                },
            },
        }
    </script>
<style>
        body {
            font-feature-settings: "cv11", "ss01";
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        .brutal-border {
            border: 2px solid #27272A;
        }

        .brutal-border-accent {
            border: 2px solid #3ECF8E;
        }

        .brutal-shadow-hover:hover {
            box-shadow: 6px 6px 0px 0px #3ECF8E;
            transform: translate(-2px, -2px);
        }

        .btn-brutal {
            transition: all 0.15s ease-out;
        }

        .btn-brutal:hover {
            transform: translate(-2px, -2px);
            box-shadow: 4px 4px 0px 0px #3ECF8E;
        }

        .btn-brutal:active {
            transform: translate(2px, 2px);
            box-shadow: 0px 0px 0px 0px #3ECF8E;
        }

        .modal-overlay {
            background: rgba(14, 14, 16, 0.8);
            backdrop-filter: blur(4px);
        }

        .modal-content {
            animation: modalSlideIn 0.3s ease-out;
        }

        @keyframes modalSlideIn {
            from {
                opacity: 0;
                transform: translateY(-20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Fix overlapping elements */
        .prose h1, .prose h2, .prose h3 {
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            line-height: 1.3;
        }

        .prose p {
            margin-top: 1em;
            margin-bottom: 1em;
            line-height: 1.8;
        }

        .prose pre {
            margin-top: 1.5em;
            margin-bottom: 1.5em;
            overflow-x: auto;
        }

        .prose code {
            word-wrap: break-word;
            white-space: pre-wrap;
        }

        .prose ul, .prose ol {
            margin-top: 0.75em;
            margin-bottom: 0.75em;
            padding-left: 1.5em;
        }

        .prose li {
            margin-top: 0.5em;
            margin-bottom: 0.5em;
        }

        /* Responsive fixes */
        @media (max-width: 768px) {
            .prose h1 { font-size: 1.75rem; }
            .prose h2 { font-size: 1.5rem; }
            .prose p { font-size: 0.95rem; }
        }
    </style>
</head>
<body class="bg-bg text-foreground antialiased selection:bg-primary/20 selection:text-foreground min-h-screen flex flex-col font-serif relative">
<!-- Grain Texture Overlay -->
<div class="fixed inset-0 pointer-events-none z-50 bg-grain mix-blend-multiply opacity-40"></div>
<!-- Top Navigation (Minimal) -->
<header class="sticky top-0 z-40 w-full border-b border-border bg-bg-secondary/95 backdrop-blur-sm h-14 flex items-center justify-between px-6 lg:px-10">
<div class="flex items-center gap-4">
<a class="flex items-center gap-2 text-foreground hover:text-accent transition-colors group" href="index.html">
<span class="material-symbols-outlined text-[20px] group-hover:-translate-x-1 transition-transform">arrow_back</span>
<span class="font-display font-medium text-lg tracking-tight">Syllabus</span>
</a>
<div class="h-4 w-px bg-faded mx-2"></div>
<span class="font-mono text-xs uppercase tracking-wider text-muted">Module 13</span>
</div>
<div class="flex items-center gap-6">
<div class="hidden md:flex items-center gap-2 text-muted text-xs font-mono">
<span class="material-symbols-outlined text-[16px]">schedule</span>
<span id="reading-time">50 min read</span>
</div>
<div class="flex gap-3">
<button id="search-btn" aria-label="Search" class="flex items-center justify-center size-8 rounded hover:bg-bg-secondary transition-colors text-foreground">
<span class="material-symbols-outlined text-[20px]">search</span>
</button>
<button aria-label="Settings" class="flex items-center justify-center size-8 rounded hover:bg-bg-secondary transition-colors text-foreground">
<span class="material-symbols-outlined text-[20px]">text_fields</span>
</button>
</div>
</div>
</header>
<!-- Main Content Layout -->
<main class="flex-1 flex justify-center w-full relative">
<div class="w-full max-w-[1440px] flex flex-row">
<!-- Left Rail: Table of Contents -->
<aside class="hidden lg:flex w-[240px] flex-col sticky top-14 h-[calc(100vh-3.5rem)] border-r border-border overflow-y-auto pt-12 pb-10 pl-10 pr-6">
<nav class="flex flex-col gap-8">
<div>
<h4 class="font-mono text-xs uppercase tracking-widest text-muted mb-4">Contents</h4>
<ul class="flex flex-col gap-3 font-mono text-[13px] leading-relaxed">
<li>
<a class="text-foreground hover:text-accent transition-colors flex items-start gap-2 group" href="#introduction">
<span class="text-accent font-medium opacity-0 group-hover:opacity-100 transition-opacity absolute -left-4">→</span>
                                    Introduction
                                </a>
</li>
<li>
<a class="text-foreground hover:text-accent transition-colors flex items-start gap-2 group" href="#domain-specific-embeddings">
<span class="text-accent font-medium opacity-0 group-hover:opacity-100 transition-opacity absolute -left-4">→</span>
                                    Domain-Specific Embeddings
                                </a>
</li>
<li>
<a class="text-foreground hover:text-accent transition-colors flex items-start gap-2 group" href="#fine-tuning-techniques">
<span class="text-accent font-medium opacity-0 group-hover:opacity-100 transition-opacity absolute -left-4">→</span>
                                    Fine-Tuning Techniques
                                </a>
</li>
<li>
<a class="text-foreground hover:text-accent transition-colors flex items-start gap-2 group" href="#adapter-methods">
<span class="text-accent font-medium opacity-0 group-hover:opacity-100 transition-opacity absolute -left-4">→</span>
                                    Adapter Methods
                                </a>
</li>
<li>
<a class="text-foreground hover:text-accent transition-colors flex items-start gap-2 group" href="#evaluation">
<span class="text-accent font-medium opacity-0 group-hover:opacity-100 transition-opacity absolute -left-4">→</span>
                                    Evaluation Strategies
                                </a>
</li>
</ul>
</div>
<div class="mt-auto pt-8 border-t border-border">
<div class="flex flex-col gap-2">
<span class="font-mono text-[11px] uppercase tracking-widest text-muted">Progress</span>
<div class="w-full bg-bg-secondary h-1 rounded-full overflow-hidden">
<div class="bg-primary h-full w-[35%]"></div>
</div>
<span class="font-mono text-xs text-foreground text-right">35%</span>
</div>
</div>
</nav>
</aside>
<!-- Center Stage: The Reader -->
<article class="flex-1 max-w-[720px] mx-auto pt-16 pb-32 px-6 md:px-12 min-h-screen">
<!-- Module Header -->
<header class="mb-16 border-b border-border pb-12">
<div class="flex items-center gap-3 mb-6">
<span class="font-mono text-sm font-medium text-accent px-2 py-1 bg-primary/10 rounded">Module 13</span>
<span class="font-mono text-sm text-muted">Advanced RAG Architectures</span>
</div>
<h1 class="font-display text-[48px] md:text-[56px] leading-[1.1] font-semibold text-foreground tracking-tight mb-6">
                        Domain Adaptation for RAG Systems
                    </h1>
<p class="font-serif text-xl text-foreground/80 leading-relaxed max-w-[90%]">
                        Techniques for adapting RAG systems to specific domains, including domain-specific embeddings, fine-tuning approaches, and evaluation methodologies.
                    </p>
</header>
<!-- Content Body -->
<div class="prose-academic">
<h2 id="introduction">Introduction to Domain Adaptation</h2>
<p>
Domain adaptation in Retrieval-Augmented Generation (RAG) systems is the process of tailoring a general-purpose RAG model to perform optimally within a specific domain or knowledge area. While general RAG systems excel at handling diverse queries, they often underperform when applied to specialized domains such as legal, medical, financial, or scientific contexts where terminology, concepts, and document structures differ significantly from general knowledge.
</p>

<div class="my-10 border border-border bg-bg-secondary/50 p-6 rounded-sm">
<figure>
<img alt="Domain Adaptation Concept" class="w-full h-auto mix-blend-multiply mb-4 filter sepia-[0.3]" data-alt="Visualization showing how a general RAG system is adapted to a specific domain like medical or legal" src="https://placehold.co/600x300/f9f7f1/ebe6da?text=Domain+Adaptation+Concept" />
<figcaption class="font-mono text-xs text-muted text-center mt-2">Figure 1: General RAG vs Domain-Adapted RAG</figcaption>
</figure>
</div>

<p>
The core challenge in domain adaptation lies in bridging the gap between general knowledge and domain-specific expertise. This involves not only adapting the retrieval mechanism to understand domain-specific terminology but also ensuring that the generation component can produce responses that align with domain conventions and expectations.
</p>

<h2 id="domain-specific-embeddings">Domain-Specific Embeddings</h2>
<p>
Creating embeddings tailored to a specific domain is often the first step in domain adaptation. Domain-specific embeddings better capture the semantic relationships within specialized vocabularies and document structures.
</p>

<h3>Training Domain-Specific Embedding Models</h3>
<p>
While general-purpose embedding models like Sentence-BERT perform well on general tasks, they may struggle with domain-specific terminology. Training or fine-tuning embedding models on domain-specific corpora can significantly improve retrieval quality.
</p>

<div class="not-prose my-12 border border-border bg-bg-secondary rounded-sm overflow-hidden">
<div class="flex items-center justify-between px-4 py-2 border-b border-border bg-[#e6dfd1]">
<span class="font-mono text-xs text-foreground">domain_embeddings.py</span>
<div class="flex gap-2">
<span class="text-[10px] font-mono text-muted uppercase">Python</span>
<button class="text-muted hover:text-accent transition-colors" onclick="copyCode(this)">
<span class="material-symbols-outlined text-[16px]">content_copy</span>
</button>
</div>
</div>
<div class="p-5 overflow-x-auto">
<pre class="font-code text-[13px] leading-6"><code class="language-python"><span class="token keyword">from</span> sentence_transformers <span class="token keyword">import</span> SentenceTransformer<span class="token punctuation">,</span> InputExample<span class="token punctuation">,</span> losses
<span class="token keyword">from</span> torch.utils.data <span class="token keyword">import</span> DataLoader
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

<span class="token keyword">class</span> <span class="token class-name">DomainEmbeddingAdapter</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> base_model_name<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">"all-MiniLM-L6-v2"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Load base embedding model</span>
        self<span class="token punctuation">.</span>base_model <span class="token operator">=</span> SentenceTransformer<span class="token punctuation">(</span>base_model_name<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>domain_model <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">prepare_training_data</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> domain_texts<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">,</span> 
                               domain_labels<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Prepare training data for domain adaptation</span>
        <span class="token keyword">if</span> domain_labels <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># Create pseudo-labels if not provided</span>
            domain_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"domain"</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> domain_texts<span class="token punctuation">]</span>

        <span class="token comment"># Create training examples</span>
        train_examples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>domain_texts<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># Create positive pairs from similar domain texts</span>
            train_examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                InputExample<span class="token punctuation">(</span>texts<span class="token operator">=</span><span class="token punctuation">[</span>domain_texts<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> domain_texts<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            
        <span class="token keyword">return</span> train_examples

    <span class="token keyword">def</span> <span class="token function">fine_tune_embeddings</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> train_examples<span class="token punctuation">,</span> epochs<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">,</span> 
                              batch_size<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Define training loss</span>
        train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_examples<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
        train_loss <span class="token operator">=</span> losses<span class="token punctuation">.</span>CosineSimilarityLoss<span class="token punctuation">(</span>self<span class="token punctuation">.</span>base_model<span class="token punctuation">)</span>

        <span class="token comment"># Fine-tune the model</span>
        self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>
            train_objectives<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">,</span> train_loss<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            epochs<span class="token operator">=</span>epochs<span class="token punctuation">,</span>
            warmup_steps<span class="token operator">=</span><span class="token number">100</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>domain_model <span class="token operator">=</span> self<span class="token punctuation">.</span>base_model
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>domain_model

    <span class="token keyword">def</span> <span class="token function">encode_texts</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> texts<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>domain_model <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> <span class="token builtin">ValueError</span><span class="token punctuation">(</span><span class="token string">"Model not trained yet. Call fine_tune_embeddings first."</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>domain_model<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>texts<span class="token punctuation">)</span>

<span class="token comment"># Example usage</span>
<span class="token comment"># adapter = DomainEmbeddingAdapter("all-MiniLM-L6-v2")</span>
<span class="token comment"># domain_texts = ["Patient presents with symptoms...", "Medical diagnosis procedure...", ...]</span>
<span class="token comment"># train_examples = adapter.prepare_training_data(domain_texts)</span>
<span class="token comment"># domain_model = adapter.fine_tune_embeddings(train_examples)</span></code></pre>
</div>
</div>

<h3>Embedding Alignment Techniques</h3>
<p>
Embedding alignment techniques help bridge the gap between general and domain-specific representations by mapping embeddings from different spaces into a unified representation.
</p>

<div class="not-prose my-12 border border-border bg-bg-secondary rounded-sm overflow-hidden">
<div class="flex items-center justify-between px-4 py-2 border-b border-border bg-[#e6dfd1]">
<span class="font-mono text-xs text-foreground">alignment.py</span>
<div class="flex gap-2">
<span class="text-[10px] font-mono text-muted uppercase">Python</span>
<button class="text-muted hover:text-accent transition-colors" onclick="copyCode(this)">
<span class="material-symbols-outlined text-[16px]">content_copy</span>
</button>
</div>
</div>
<div class="p-5 overflow-x-auto">
<pre class="font-code text-[13px] leading-6"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> sklearn.linear_model <span class="token keyword">import</span> LinearRegression
<span class="token keyword">from</span> sklearn.preprocessing <span class="token keyword">import</span> StandardScaler
<span class="token keyword">from</span> typing <span class="token keyword">import</span> List

<span class="token keyword">class</span> <span class="token class-name">EmbeddingAligner</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>alignment_matrix <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>source_scaler <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>target_scaler <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">align_embeddings</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> source_embeddings<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> 
                             target_embeddings<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Align source embeddings to target embeddings space</span>
        <span class="token comment"># Scale both embedding sets</span>
        scaled_source <span class="token operator">=</span> self<span class="token punctuation">.</span>source_scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>source_embeddings<span class="token punctuation">)</span>
        scaled_target <span class="token operator">=</span> self<span class="token punctuation">.</span>target_scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>target_embeddings<span class="token punctuation">)</span>

        <span class="token comment"># Learn transformation matrix using linear regression</span>
        <span class="token comment"># Each dimension of target is predicted from all dimensions of source</span>
        self<span class="token punctuation">.</span>alignment_matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>pinv<span class="token punctuation">(</span>scaled_source<span class="token punctuation">.</span>T <span class="token operator">@</span> scaled_source<span class="token punctuation">)</span> <span class="token operator">@</span> scaled_source<span class="token punctuation">.</span>T <span class="token operator">@</span> scaled_target
        
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>alignment_matrix

    <span class="token keyword">def</span> <span class="token function">transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embeddings<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>alignment_matrix <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> <span class="token builtin">ValueError</span><span class="token punctuation">(</span><span class="token string">"Alignment matrix not computed. Call align_embeddings first."</span><span class="token punctuation">)</span>

        <span class="token comment"># Transform embeddings using learned alignment</span>
        scaled_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>source_scaler<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>
        aligned_embeddings <span class="token operator">=</span> scaled_embeddings <span class="token operator">@</span> self<span class="token punctuation">.</span>alignment_matrix
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>target_scaler<span class="token punctuation">.</span>inverse_transform<span class="token punctuation">(</span>aligned_embeddings<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">compute_alignment_score</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> source_embeddings<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> 
                                   target_embeddings<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> float<span class="token punctuation">:</span>
        <span class="token comment"># Compute alignment score between source and target embeddings</span>
        aligned_source <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>source_embeddings<span class="token punctuation">)</span>
        cosine_similarities <span class="token operator">=</span> <span class="token punctuation">(</span>aligned_source <span class="token operator">*</span> target_embeddings<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> \
                              <span class="token punctuation">(</span>np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>aligned_source<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> 
                               np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>target_embeddings<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> cosine_similarities<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Example usage</span>
<span class="token comment"># aligner = EmbeddingAligner()</span>
<span class="token comment"># alignment_matrix = aligner.align_embeddings(general_embeddings, domain_embeddings)</span>
<span class="token comment"># aligned_general = aligner.transform(general_embeddings)</span></code></pre>
</div>
</div>

<h2 id="fine-tuning-techniques">Fine-Tuning Approaches</h2>
<p>
Fine-tuning is a powerful technique for adapting pre-trained models to domain-specific tasks. Various approaches exist, each with different trade-offs between computational requirements and effectiveness.
</p>

<h3>LoRA (Low-Rank Adaptation)</h3>
<p>
LoRA is an efficient fine-tuning technique that adds trainable low-rank decomposition matrices to the original model weights, significantly reducing the number of parameters that need to be updated.
</p>

<div class="not-prose my-12 border border-border bg-bg-secondary rounded-sm overflow-hidden">
<div class="flex items-center justify-between px-4 py-2 border-b border-border bg-[#e6dfd1]">
<span class="font-mono text-xs text-foreground">lora_finetuning.py</span>
<div class="flex gap-2">
<span class="text-[10px] font-mono text-muted uppercase">Python</span>
<button class="text-muted hover:text-accent transition-colors" onclick="copyCode(this)">
<span class="material-symbols-outlined text-[16px]">content_copy</span>
</button>
</div>
</div>
<div class="p-5 overflow-x-auto">
<pre class="font-code text-[13px] leading-6"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer
<span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig<span class="token punctuation">,</span> get_peft_model<span class="token punctuation">,</span> TaskType

<span class="token keyword">class</span> <span class="token class-name">LoRADomainAdapter</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_name<span class="token punctuation">:</span> str<span class="token punctuation">,</span> 
                 r<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span> 
                 lora_alpha<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> 
                 lora_dropout<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>model_name <span class="token operator">=</span> model_name
        self<span class="token punctuation">.</span>r <span class="token operator">=</span> r
        self<span class="token punctuation">.</span>lora_alpha <span class="token operator">=</span> lora_alpha
        self<span class="token punctuation">.</span>lora_dropout <span class="token operator">=</span> lora_dropout
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">setup_lora_model</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Load base model and tokenizer</span>
        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_name<span class="token punctuation">)</span>
        
        <span class="token comment"># Add padding token if not present</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>pad_token <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>pad_token <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>eos_token

        self<span class="token punctuation">.</span>model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>model_name<span class="token punctuation">,</span>
            torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
            device_map<span class="token operator">=</span><span class="token string">"auto"</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># Configure LoRA</span>
        peft_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
            task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">,</span>
            inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
            r<span class="token operator">=</span>self<span class="token punctuation">.</span>r<span class="token punctuation">,</span>
            lora_alpha<span class="token operator">=</span>self<span class="token punctuation">.</span>lora_alpha<span class="token punctuation">,</span>
            lora_dropout<span class="token operator">=</span>self<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">,</span>
            target_modules<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">,</span> <span class="token string">"k_proj"</span><span class="token punctuation">,</span> <span class="token string">"o_proj"</span><span class="token punctuation">,</span> 
                          <span class="token string">"gate_proj"</span><span class="token punctuation">,</span> <span class="token string">"up_proj"</span><span class="token punctuation">,</span> <span class="token string">"down_proj"</span><span class="token punctuation">,</span> <span class="token string">"lm_head"</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># Apply LoRA to the model</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model

    <span class="token keyword">def</span> <span class="token function">prepare_training_data</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> domain_prompts<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">,</span> 
                               domain_responses<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Format training data for instruction tuning</span>
        formatted_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> prompt<span class="token punctuation">,</span> response <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>domain_prompts<span class="token punctuation">,</span> domain_responses<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># Format as instruction-response pairs</span>
            formatted_entry <span class="token operator">=</span> <span class="token punctuation">{</span>
                <span class="token string">"input_ids"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">(</span>
                    f<span class="token string">"### Instruction:\n{prompt}\n\n### Response:\n{response}\n\n### End"</span><span class="token punctuation">,</span>
                    <span class="token builtin">max</span>_length<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>
                    truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                    padding<span class="token operator">=</span><span class="token boolean">False</span>
                <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
            <span class="token punctuation">}</span>
            formatted_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>formatted_entry<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> formatted_data

    <span class="token keyword">def</span> <span class="token function">fine_tune</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> train_dataset<span class="token punctuation">,</span> 
                     epochs<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span> 
                     learning_rate<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">5e-5</span><span class="token punctuation">,</span> 
                     batch_size<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments<span class="token punctuation">,</span> Trainer
        
        <span class="token comment"># Setup training arguments</span>
        training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
            output_dir<span class="token operator">=</span><span class="token string">"./domain_adapter_output"</span><span class="token punctuation">,</span>
            overwrite_output_dir<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
            num_train_epochs<span class="token operator">=</span>epochs<span class="token punctuation">,</span>
            per_device_train_batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>
            gradient_accumulation_steps<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
            warmup_steps<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
            learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span>
            logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
            save_strategy<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">,</span>
            fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment"># Use mixed precision for efficiency</span>
            report_to<span class="token operator">=</span><span class="token string">"none"</span>  <span class="token comment"># Disable reporting to external services</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># Setup trainer</span>
        trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
            model<span class="token operator">=</span>self<span class="token punctuation">.</span>model<span class="token punctuation">,</span>
            args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
            train_dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span>
            tokenizer<span class="token operator">=</span>self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># Start training</span>
        trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># Save the fine-tuned model</span>
        trainer<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model

<span class="token comment"># Example usage</span>
<span class="token comment"># adapter = LoRADomainAdapter("microsoft/DialoGPT-medium")</span>
<span class="token comment"># model = adapter.setup_lora_model()</span>
<span class="token comment"># train_data = adapter.prepare_training_data(prompts, responses)</span>
<span class="token comment"># fine_tuned_model = adapter.fine_tune(train_data)</span></code></pre>
</div>
</div>

<h2 id="adapter-methods">Adapter Methods</h2>
<p>
Adapter methods insert small neural networks within pre-trained models to enable domain adaptation without modifying the original parameters extensively.
</p>

<h3>Universal Language Model Adapters</h3>
<p>
Universal adapters are lightweight modules inserted at each transformer layer that can be trained for specific tasks while preserving the original model's capabilities.
</p>

<div class="not-prose my-12 border border-border bg-bg-secondary rounded-sm overflow-hidden">
<div class="flex items-center justify-between px-4 py-2 border-b border-border bg-[#e6dfd1]">
<span class="font-mono text-xs text-foreground">universal_adapters.py</span>
<div class="flex gap-2">
<span class="text-[10px] font-mono text-muted uppercase">Python</span>
<button class="text-muted hover:text-accent transition-colors" onclick="copyCode(this)">
<span class="material-symbols-outlined text-[16px]">content_copy</span>
</button>
</div>
</div>
<div class="p-5 overflow-x-auto">
<pre class="font-code text-[13px] leading-6"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModel
<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional

<span class="token keyword">class</span> <span class="token class-name">UniversalAdapter</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">:</span> int<span class="token punctuation">,</span> bottleneck_size<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>bottleneck_size <span class="token operator">=</span> bottleneck_size
        
        <span class="token comment"># Down-project to bottleneck</span>
        self<span class="token punctuation">.</span>down_project <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> bottleneck_size<span class="token punctuation">)</span>
        <span class="token comment"># Non-linearity</span>
        self<span class="token punctuation">.</span>activation <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># Up-project back to hidden size</span>
        self<span class="token punctuation">.</span>up_project <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>bottleneck_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        
        <span class="token comment"># Initialize weights</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>down_project<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>down_project<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>up_project<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>up_project<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token comment"># Store residual connection</span>
        residual <span class="token operator">=</span> x
        
        <span class="token comment"># Down-project</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>down_project<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># Apply activation</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>activation<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># Up-project</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up_project<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        <span class="token comment"># Add residual connection</span>
        <span class="token keyword">return</span> x <span class="token operator">+</span> residual

<span class="token keyword">class</span> <span class="token class-name">AdapterLayer</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">,</span> hidden_size<span class="token punctuation">:</span> int<span class="token punctuation">,</span> adapter_dim<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> layer
        self<span class="token punctuation">.</span>adapter <span class="token operator">=</span> UniversalAdapter<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> adapter_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>adapter<span class="token punctuation">.</span>scale <span class="token operator">=</span> <span class="token number">1.0</span>  <span class="token comment"># Scaling factor for adapter output</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> **kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token comment"># Forward through original layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> **kwargs<span class="token punctuation">)</span>
        
        <span class="token comment"># Apply adapter to the hidden states</span>
        <span class="token comment"># Usually applied after the feed-forward block</span>
        adapter_output <span class="token operator">=</span> self<span class="token punctuation">.</span>adapter<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>adapter<span class="token punctuation">.</span>scale
        
        <span class="token comment"># Return modified output</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>adapter_output<span class="token punctuation">,</span> <span class="token operator">*</span>output<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">DomainAdaptedModel</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> base_model_name<span class="token punctuation">:</span> str<span class="token punctuation">,</span> adapter_dim<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>base_model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>base_model_name<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>adapter_dim <span class="token operator">=</span> adapter_dim
        self<span class="token punctuation">.</span>adapters_added <span class="token operator">=</span> <span class="token boolean">False</span>

    <span class="token keyword">def</span> <span class="token function">add_adapters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Add adapters to each transformer layer</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> layer <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># Wrap the layer with an adapter</span>
            adapter_layer <span class="token operator">=</span> AdapterLayer<span class="token punctuation">(</span>
                layer<span class="token punctuation">,</span> 
                self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> 
                self<span class="token punctuation">.</span>adapter_dim
            <span class="token punctuation">)</span>
            
            <span class="token comment"># Replace the original layer with the adapter-wrapped layer</span>
            self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layer<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> adapter_layer
            
        self<span class="token punctuation">.</span>adapters_added <span class="token operator">=</span> <span class="token boolean">True</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>base_model

    <span class="token keyword">def</span> <span class="token function">fine_tune_adapters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> train_dataset<span class="token punctuation">,</span> epochs<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Only train adapter parameters, freeze base model</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
            
        <span class="token comment"># Enable gradients only for adapter parameters</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token string">"adapter"</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
                param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

        <span class="token comment"># Training logic would go here</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Adapter fine-tuning initialized. Only adapter parameters will be updated."</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>base_model

<span class="token comment"># Example usage</span>
<span class="token comment"># model = DomainAdaptedModel("bert-base-uncased")</span>
<span class="token comment"># model_with_adapters = model.add_adapters()</span>
<span class="token comment"># fine_tuned_model = model.fine_tune_adapters(train_data)</span></code></pre>
</div>
</div>

<h2 id="evaluation">Evaluation Strategies for Domain Adaptation</h2>
<p>
Evaluating domain-adapted RAG systems requires specialized metrics and benchmarks that reflect the specific domain's requirements and challenges.
</p>

<h3>Domain-Specific Evaluation Metrics</h3>
<p>
Traditional metrics like BLEU or ROUGE may not adequately capture the quality of domain-specific responses. Custom evaluation metrics tailored to the domain are often necessary.
</p>

<div class="not-prose my-12 border border-border bg-bg-secondary rounded-sm overflow-hidden">
<div class="flex items-center justify-between px-4 py-2 border-b border-border bg-[#e6dfd1]">
<span class="font-mono text-xs text-foreground">evaluation_metrics.py</span>
<div class="flex gap-2">
<span class="text-[10px] font-mono text-muted uppercase">Python</span>
<button class="text-muted hover:text-accent transition-colors" onclick="copyCode(this)">
<span class="material-symbols-outlined text-[16px]">content_copy</span>
</button>
</div>
</div>
<div class="p-5 overflow-x-auto">
<pre class="font-code text-[13px] leading-6"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token punctuation">,</span> Dict<span class="token punctuation">,</span> Tuple
<span class="token keyword">from</span> sklearn.metrics <span class="token keyword">import</span> f1_score<span class="token punctuation">,</span> precision_score<span class="token punctuation">,</span> recall_score

<span class="token keyword">class</span> <span class="token class-name">DomainEvaluator</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> domain_keywords<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">,</span> expert_annotations<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Dict<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>domain_keywords <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>domain_keywords<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>expert_annotations <span class="token operator">=</span> expert_annotations

    <span class="token keyword">def</span> <span class="token function">calculate_domain_relevance</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">:</span> str<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> float<span class="token punctuation">:</span>
        <span class="token comment"># Calculate how many domain-specific keywords appear in the response</span>
        response_words <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        matching_keywords <span class="token operator">=</span> self<span class="token punctuation">.</span>domain_keywords <span class="token operator">&amp;</span> response_words
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>matching_keywords<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>domain_keywords<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">calculate_coherence_score</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">:</span> str<span class="token punctuation">,</span> reference<span class="token punctuation">:</span> str<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> float<span class="token punctuation">:</span>
        <span class="token comment"># Simple n-gram overlap as a proxy for coherence</span>
        <span class="token keyword">def</span> <span class="token function">get_bigrams</span><span class="token punctuation">(</span>text<span class="token punctuation">:</span> str<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">set</span><span class="token punctuation">:</span>
            words <span class="token operator">=</span> text<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>words<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        response_bigrams <span class="token operator">=</span> get_bigrams<span class="token punctuation">(</span>response<span class="token punctuation">)</span>
        reference_bigrams <span class="token operator">=</span> get_bigrams<span class="token punctuation">(</span>reference<span class="token punctuation">)</span>
        
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>response_bigrams<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token number">0.0</span>
        
        <span class="token comment"># Jaccard similarity of bigrams</span>
        intersection <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>response_bigrams <span class="token operator">&amp;</span> reference_bigrams<span class="token punctuation">)</span>
        union <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>response_bigrams <span class="token operator">|</span> reference_bigrams<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> intersection <span class="token operator">/</span> union <span class="token keyword">if</span> union <span class="token operator">></span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token number">0.0</span>

    <span class="token keyword">def</span> <span class="token function">evaluate_response_quality</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> responses<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">,</span> 
                                      references<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">,</span> 
                                      queries<span class="token punctuation">:</span> List<span class="token punctuation">[</span>str<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Dict<span class="token punctuation">[</span>str<span class="token punctuation">,</span> float<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token comment"># Comprehensive evaluation of response quality</span>
        domain_relevance_scores <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>calculate_domain_relevance<span class="token punctuation">(</span>resp<span class="token punctuation">)</span> <span class="token keyword">for</span> resp <span class="token keyword">in</span> responses<span class="token punctuation">]</span>
        coherence_scores <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>calculate_coherence_score<span class="token punctuation">(</span>resp<span class="token punctuation">,</span> ref<span class="token punctuation">)</span> 
                            <span class="token keyword">for</span> resp<span class="token punctuation">,</span> ref <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>responses<span class="token punctuation">,</span> references<span class="token punctuation">)</span><span class="token punctuation">]</span>
        
        avg_domain_relevance <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>domain_relevance_scores<span class="token punctuation">)</span>
        avg_coherence <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>coherence_scores<span class="token punctuation">)</span>
        
        <span class="token comment"># Overall score as weighted average</span>
        overall_score <span class="token operator">=</span> <span class="token number">0.6</span> <span class="token operator">*</span> avg_domain_relevance <span class="token operator">+</span> <span class="token number">0.4</span> <span class="token operator">*</span> avg_coherence
        
        <span class="token keyword">return</span> <span class="token punctuation">{</span>
            <span class="token string">'domain_relevance'</span><span class="token punctuation">:</span> avg_domain_relevance<span class="token punctuation">,</span>
            <span class="token string">'coherence'</span><span class="token punctuation">:</span> avg_coherence<span class="token punctuation">,</span>
            <span class="token string">'overall_score'</span><span class="token punctuation">:</span> overall_score<span class="token punctuation">,</span>
            <span class="token string">'individual_domain_scores'</span><span class="token punctuation">:</span> domain_relevance_scores<span class="token punctuation">,</span>
            <span class="token string">'individual_coherence_scores'</span><span class="token punctuation">:</span> coherence_scores
        <span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">evaluate_retrieval_quality</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> retrieved_docs<span class="token punctuation">:</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                                     ground_truth_docs<span class="token punctuation">:</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span>str<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Dict<span class="token punctuation">[</span>str<span class="token punctuation">,</span> float<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token comment"># Evaluate retrieval quality using domain-specific metrics</span>
        <span class="token comment"># Calculate precision, recall, and F1 for domain keyword coverage</span>
        
        precisions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        recalls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        
        <span class="token keyword">for</span> retrieved<span class="token punctuation">,</span> ground_truth <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>retrieved_docs<span class="token punctuation">,</span> ground_truth_docs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># Combine all retrieved and ground truth documents</span>
            retrieved_text <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>retrieved<span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>
            ground_truth_text <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>ground_truth<span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            <span class="token comment"># Extract domain keywords from texts</span>
            retrieved_keywords <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>retrieved_text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> self<span class="token punctuation">.</span>domain_keywords
            ground_truth_keywords <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>ground_truth_text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> self<span class="token punctuation">.</span>domain_keywords
            
            <span class="token comment"># Calculate precision and recall for domain keywords</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>retrieved_keywords<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
                precision <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>retrieved_keywords <span class="token operator">&amp;</span> ground_truth_keywords<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>retrieved_keywords<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                precision <span class="token operator">=</span> <span class="token number">0.0</span>
                
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>ground_truth_keywords<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
                recall <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>retrieved_keywords <span class="token operator">&amp;</span> ground_truth_keywords<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>ground_truth_keywords<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                recall <span class="token operator">=</span> <span class="token number">0.0</span>
                
            precisions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>precision<span class="token punctuation">)</span>
            recalls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>recall<span class="token punctuation">)</span>
        
        <span class="token comment"># Calculate macro averages</span>
        avg_precision <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>precisions<span class="token punctuation">)</span>
        avg_recall <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>recalls<span class="token punctuation">)</span>
        f1 <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>avg_precision <span class="token operator">*</span> avg_recall<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>avg_precision <span class="token operator">+</span> avg_recall<span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>avg_precision <span class="token operator">+</span> avg_recall<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token number">0.0</span>
        
        <span class="token keyword">return</span> <span class="token punctuation">{</span>
            <span class="token string">'precision'</span><span class="token punctuation">:</span> avg_precision<span class="token punctuation">,</span>
            <span class="token string">'recall'</span><span class="token punctuation">:</span> avg_recall<span class="token punctuation">,</span>
            <span class="token string">'f1'</span><span class="token punctuation">:</span> f1<span class="token punctuation">,</span>
            <span class="token string">'individual_precisions'</span><span class="token punctuation">:</span> precisions<span class="token punctuation">,</span>
            <span class="token string">'individual_recalls'</span><span class="token punctuation">:</span> recalls
        <span class="token punctuation">}</span>

<span class="token comment"># Example usage</span>
<span class="token comment"># domain_keywords = ["patient", "diagnosis", "symptoms", "treatment", "medication"]</span>
<span class="token comment"># evaluator = DomainEvaluator(domain_keywords)</span>
<span class="token comment"># quality_metrics = evaluator.evaluate_response_quality(responses, references)</span>
<span class="token comment"># retrieval_metrics = evaluator.evaluate_retrieval_quality(retrieved_docs, ground_truth_docs)</span></code></pre>
</div>
</div>

</div>
<!-- Footer / Pagination -->
<div class="mt-24 pt-10 border-t border-border flex justify-between items-center group/footer">
<a class="flex flex-col items-start gap-2 hover:bg-bg-secondary/50 p-4 -ml-4 rounded transition-colors w-1/2" href="module_12_optimization_techniques.html">
<span class="font-mono text-xs text-muted uppercase tracking-wider">Previous Module</span>
<span class="font-display text-lg font-medium text-foreground flex items-center gap-2">
<span class="material-symbols-outlined text-[18px]">arrow_back</span>
                            Optimization Techniques
                        </span>
</a>
<div class="h-12 w-px bg-faded mx-4"></div>
<a class="flex flex-col items-end gap-2 hover:bg-bg-secondary/50 p-4 -mr-4 rounded transition-colors w-1/2" href="module_14_advanced_architectures.html">
<span class="font-mono text-xs text-muted uppercase tracking-wider">Next Module</span>
<span class="font-display text-lg font-medium text-foreground flex items-center gap-2">
                            Advanced Architectures
                            <span class="material-symbols-outlined text-[18px]">arrow_forward</span>
</span>
</a>
</div>
</article>
<!-- Right Rail: Marginalia -->
<aside class="hidden xl:block w-[240px] sticky top-14 h-[calc(100vh-3.5rem)] pt-32 pb-10 pr-10 pl-4 overflow-y-auto">
<div class="flex flex-col gap-24 relative">
<!-- Note 1: Aligned with introduction -->
<div class="relative group cursor-pointer transition-transform hover:-translate-x-1">
<div class="absolute -left-3 top-1 text-[10px] font-bold text-accent font-mono">[1]</div>
<p class="font-mono text-[12px] leading-5 text-muted group-hover:text-foreground transition-colors">
<strong class="text-foreground">Domain adaptation</strong> is crucial for specialized RAG applications.
                        </p>
</div>
<!-- Note 2: Aligned with domain-specific embeddings -->
<div class="relative group cursor-pointer transition-transform hover:-translate-x-1 mt-12">
<div class="absolute -left-3 top-1 text-[10px] font-bold text-accent font-mono">[2]</div>
<p class="font-mono text-[12px] leading-5 text-muted group-hover:text-foreground transition-colors">
<strong class="text-foreground">Domain-specific embeddings</strong> better capture specialized terminology.
                        </p>
<a class="inline-flex items-center gap-1 mt-2 text-[10px] uppercase tracking-widest text-accent font-bold hover:underline" href="artifact_view.html" target="_blank">
                            View Code <span class="material-symbols-outlined text-[10px]">open_in_new</span>
</a>
</div>
<!-- Note 3: Aligned with fine-tuning techniques -->
<div class="relative group cursor-pointer transition-transform hover:-translate-x-1 mt-24">
<div class="absolute -left-3 top-1 text-[10px] font-bold text-accent font-mono">[3]</div>
<p class="font-mono text-[12px] leading-5 text-muted group-hover:text-foreground transition-colors">
                            <code class="bg-bg-secondary px-0.5 rounded-sm">LoRA</code> enables efficient fine-tuning with minimal parameters.
                        </p>
</div>
<!-- Additional marginalia for module-specific content -->
<div class="relative group cursor-pointer transition-transform hover:-translate-x-1 mt-16">
<div class="absolute -left-3 top-1 text-[10px] font-bold text-accent font-mono">[4]</div>
<p class="font-mono text-[12px] leading-5 text-muted group-hover:text-foreground transition-colors">
                            <strong class="text-foreground">Universal adapters</strong> allow for modular domain adaptation.
                        </p>
</div>
<div class="relative group cursor-pointer transition-transform hover:-translate-x-1 mt-16">
<div class="absolute -left-3 top-1 text-[10px] font-bold text-accent font-mono">[5]</div>
<p class="font-mono text-[12px] leading-5 text-muted group-hover:text-foreground transition-colors">
                            <strong class="text-foreground">Custom evaluation metrics</strong> are essential for domain-specific quality assessment.
                        </p>
</div>
</div>
</aside>
</div>
</main>

<script>
    // Function to copy code to clipboard
    function copyCode(button) {
        const codeBlock = button.closest('.not-prose').querySelector('pre code');
        navigator.clipboard.writeText(codeBlock.textContent.trim()).then(() => {
            const originalIcon = button.querySelector('.material-symbols-outlined').textContent;
            button.querySelector('.material-symbols-outlined').textContent = 'check';
            setTimeout(() => {
                button.querySelector('.material-symbols-outlined').textContent = originalIcon;
            }, 2000);
        });
    }

    // Calculate reading time based on content
    document.addEventListener('DOMContentLoaded', function() {
        const content = document.querySelector('.prose-academic').textContent;
        const wordsPerMinute = 200; // Average reading speed
        const wordCount = content.split(/\s+/).length;
        const readingTime = Math.ceil(wordCount / wordsPerMinute);

        document.getElementById('reading-time').textContent = `${readingTime} min read`;
    });

    // Search button functionality
    document.getElementById('search-btn').addEventListener('click', function() {
        window.location.href = 'search_index.html';
    });
</script>
</body></html>